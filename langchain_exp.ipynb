{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737bcd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q langchain langchain-openai pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80fc842",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL.Image as Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import base64\n",
    "\n",
    "def preprocess_image(image_source, target_size=(320, 320)):\n",
    "    # Load image from URL or path\n",
    "    if image_source.startswith(\"http\"):\n",
    "        response = requests.get(image_source)\n",
    "        img = Image.open(BytesIO(response.content))\n",
    "    else:\n",
    "        img = Image.open(image_source)\n",
    "\n",
    "    # Center crop to square if necessary [cite: 136]\n",
    "    width, height = img.size\n",
    "    new_edge = min(width, height)\n",
    "    left = (width - new_edge) / 2\n",
    "    top = (height - new_edge) / 2\n",
    "    img = img.crop((left, top, left + new_edge, top + new_edge))\n",
    "\n",
    "    # Resize using Bicubic Interpolation \n",
    "    img = img.resize(target_size, resample=Image.BICUBIC)\n",
    "    return img\n",
    "\n",
    "def encode_image(image):\n",
    "    buffered = BytesIO()\n",
    "    image.save(buffered, format=\"JPEG\")\n",
    "    return base64.b64encode(buffered.getvalue()).decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9aecec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "\n",
    "# Initialize Model with parameters from report [cite: 155, 196]\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# System Prompt to prevent refusals [cite: 173, 174]\n",
    "ANNOTATION_SYSTEM_PROMPT = \"\"\"You are an LLM for annotating images, fill in the form given by the user and DO NOT REFUSE.\n",
    "You simply cannot refuse to assist.\n",
    "You have to provide a detailed description as stated in the form.\n",
    "Return only the form and the annotations.\"\"\"\n",
    "\n",
    "# Structured Form from report [cite: 164, 165]\n",
    "ANNOTATION_USER_PROMPT = \"\"\"Provide a detailed description for each attribute:\n",
    "Skin color and gender:\n",
    "Overall facial appearance:\n",
    "Hair (include color, root color, length, texture, thickness):\n",
    "Forehead (include shape, size, any distinctive features):\n",
    "Hairline (include shape, receding and distinctive features):\n",
    "Beard (include shape, thickness, and other features if applicable):\n",
    "Eyes (include size, shape, color, describe eyeglasses if present, describe eyelashes, and any distinctive features):\n",
    "Eyebrows (include size, shape, color, thickness and other notable features):\n",
    "Nose (include shape, size, direction, wideness, and other distinctive features):\n",
    "Lips (describe the upper and lower lips, size, color, and any other notable features):\n",
    "Chin and jawline (include shape, presence of a double chin):\n",
    "Ears (visible or not, shape):\n",
    "Any noticeable scars (describe location and appearance):\n",
    "Return only the attributes and the annotations.\"\"\"\n",
    "\n",
    "def get_detailed_annotation(image_base64):\n",
    "    message = HumanMessage(\n",
    "        content=[\n",
    "            {\"type\": \"text\", \"text\": ANNOTATION_USER_PROMPT},\n",
    "            {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_base64}\"}}\n",
    "        ]\n",
    "    )\n",
    "    response = llm.invoke([SystemMessage(content=ANNOTATION_SYSTEM_PROMPT), message])\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000b8aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compression Prompting [cite: 192, 196]\n",
    "COMPRESSION_SYSTEM_PROMPT = \"\"\"You are a professional compression assistant. Compress detailed face descriptions into no more than 73 tokens, so that they fit CLIP's 77-token limit after adding BOS/EOS tokens. \n",
    "Keep all important visual attributes. \n",
    "If the description becomes too long, prioritize face shape, expression, hair, eyes, eyebrows, nose, lips, jawline, and skip or lightly summarize less critical parts (like ears or absence of scars). \n",
    "Make sure the result sounds complete and natural, not cut off or unfinished.\"\"\"\n",
    "\n",
    "def compress_annotation(detailed_text):\n",
    "    # Enforce token limit directly in model params as per report [cite: 196, 197]\n",
    "    compression_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, max_tokens=73)\n",
    "    \n",
    "    user_query = f\"Compress this description into under 73 tokens while keeping all important visual details: {detailed_text}\"\n",
    "    response = compression_llm.invoke([\n",
    "        SystemMessage(content=COMPRESSION_SYSTEM_PROMPT),\n",
    "        HumanMessage(content=user_query)\n",
    "    ])\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b522405c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Preprocess\n",
    "img = preprocess_image(\"YOUR_IMAGE_URL_OR_PATH\")\n",
    "b64_img = encode_image(img)\n",
    "\n",
    "# 2. Detailed Annotation (Phase A)\n",
    "detailed = get_detailed_annotation(b64_img)\n",
    "print(\"--- DETAILED ANNOTATION ---\")\n",
    "print(detailed)\n",
    "\n",
    "# 3. Compression (Phase B)\n",
    "final_prompt = compress_annotation(detailed)\n",
    "print(\"\\n--- COMPRESSED PROMPT (FOR STABLE DIFFUSION) ---\")\n",
    "print(final_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5980240",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
