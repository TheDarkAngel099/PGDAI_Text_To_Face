{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ==== Colab-Safe Face Cropper | YOLOv5-face + (CPU) SCRFD fallback ====\n",
        "# Input:  /content/images\n",
        "# Output: /content/cropped_faces\n",
        "# Aligns by eyes, square-crops, resizes to 320x320. Works on Colab Py3.12 + NumPy 2.x.\n",
        "# ----------------------------------------------------------------------\n",
        "\n",
        "# 1) Install deps compatible with current Colab (Py3.12 + NumPy 2.x)\n",
        "!pip -q install \"opencv-python-headless>=4.10.0.84\" tqdm matplotlib\n",
        "# Fallback detector only (CPU): InsightFace + onnxruntime (Py3.12-compatible)\n",
        "!pip -q install insightface==0.7.3 onnxruntime==1.20.1\n",
        "\n",
        "import os, sys, math, urllib.request, warnings\n",
        "import cv2\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# ---------- Config ----------\n",
        "INPUT_DIR  = \"/content/images\"\n",
        "OUTPUT_DIR = \"/content/cropped_faces\"\n",
        "SAVE_ALL_FACES = False      # False: only largest face per image; True: save all faces\n",
        "TARGET_SIZE = (320, 320)    # final (width, height)\n",
        "ASPECT_W, ASPECT_H = 1, 1   # enforce square\n",
        "MARGIN = 1.5                # expand around face;\n",
        "MIN_CROP_MIN_DIM = 40       # skip tiny crops\n",
        "CONF_THRES = 0.25           # YOLOv5-face confidence threshold\n",
        "IOU_THRES = 0.45            # YOLOv5-face NMS threshold\n",
        "IMG_SIZE = 640              # YOLOv5-face inference size\n",
        "# ----------------------------\n",
        "\n",
        "os.makedirs(INPUT_DIR, exist_ok=True)\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# ---------- Geometry helpers ----------\n",
        "def rotate_align(image, kps):\n",
        "    \"\"\"\n",
        "    Align image so eyes are horizontal using 5-point landmarks.\n",
        "    kps expected: [ [x_le, y_le], [x_re, y_re], nose, lm, rm ]\n",
        "    \"\"\"\n",
        "    kps = np.asarray(kps, dtype=np.float32)\n",
        "    left_eye, right_eye = kps[0], kps[1]\n",
        "    dx, dy = right_eye[0] - left_eye[0], right_eye[1] - left_eye[1]\n",
        "    angle = math.degrees(math.atan2(dy, dx))\n",
        "    center = ((left_eye[0] + right_eye[0]) / 2.0, (left_eye[1] + right_eye[1]) / 2.0)\n",
        "    M = cv2.getRotationMatrix2D((float(center[0]), float(center[1])), angle, 1.0)\n",
        "    h, w = image.shape[:2]\n",
        "    rotated = cv2.warpAffine(image, M, (w, h), flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_REPLICATE)\n",
        "    # transform landmarks\n",
        "    kps_h = np.hstack([kps, np.ones((kps.shape[0], 1), dtype=np.float32)])  # (5,3)\n",
        "    kps_rot = (M @ kps_h.T).T\n",
        "    return rotated, kps_rot\n",
        "\n",
        "def bbox_from_landmarks(kps, scale=MARGIN):\n",
        "    x_min, y_min = np.min(kps, axis=0)\n",
        "    x_max, y_max = np.max(kps, axis=0)\n",
        "    cx, cy = (x_min + x_max) / 2.0, (y_min + y_max) / 2.0\n",
        "    w, h = (x_max - x_min), (y_max - y_min)\n",
        "    w *= (1 + scale)\n",
        "    h *= (1 + scale * 1.2)  # slightly more vertical room for forehead/chin\n",
        "    return [cx - w/2, cy - h/2, cx + w/2, cy + h/2]\n",
        "\n",
        "def enforce_aspect_and_pad(img, box, aspect=(ASPECT_W, ASPECT_H)):\n",
        "    \"\"\"\n",
        "    Expand the shorter side to match target aspect, pad out-of-bounds, then crop.\n",
        "    \"\"\"\n",
        "    h, w = img.shape[:2]\n",
        "    x1, y1, x2, y2 = map(float, box)\n",
        "    bw, bh = x2 - x1, y2 - y1\n",
        "    target_ratio = aspect[0] / aspect[1]\n",
        "    curr_ratio = (bw / bh) if bh > 1e-6 else target_ratio\n",
        "\n",
        "    if curr_ratio > target_ratio:\n",
        "        # too wide -> increase height\n",
        "        new_bh = bw / target_ratio\n",
        "        delta = (new_bh - bh) / 2.0\n",
        "        y1 -= delta; y2 += delta\n",
        "    else:\n",
        "        # too tall -> increase width\n",
        "        new_bw = bh * target_ratio\n",
        "        delta = (new_bw - bw) / 2.0\n",
        "        x1 -= delta; x2 += delta\n",
        "\n",
        "    # pad out-of-bounds with replicate\n",
        "    left_pad   = max(0, int(-np.floor(x1)))\n",
        "    top_pad    = max(0, int(-np.floor(y1)))\n",
        "    right_pad  = max(0, int(np.ceil(x2) - w))\n",
        "    bottom_pad = max(0, int(np.ceil(y2) - h))\n",
        "\n",
        "    if any([left_pad, top_pad, right_pad, bottom_pad]):\n",
        "        img = cv2.copyMakeBorder(img, top_pad, bottom_pad, left_pad, right_pad, cv2.BORDER_REPLICATE)\n",
        "        x1 += left_pad; x2 += left_pad\n",
        "        y1 += top_pad;  y2 += top_pad\n",
        "\n",
        "    x1i, y1i, x2i, y2i = map(lambda v: int(round(v)), [x1, y1, x2, y2])\n",
        "    x1i = max(0, x1i); y1i = max(0, y1i)\n",
        "    x2i = min(img.shape[1], x2i); y2i = min(img.shape[0], y2i)\n",
        "    crop = img[y1i:y2i, x1i:x2i]\n",
        "    return crop\n",
        "\n",
        "def face_area_bbox(b):\n",
        "    x1,y1,x2,y2 = b\n",
        "    return max(0,(x2-x1))*max(0,(y2-y1))\n",
        "\n",
        "# ---------- YOLOv5-face (primary) ----------\n",
        "yolo_ok = False\n",
        "try:\n",
        "    YOLO_DIR = \"/content/yolov5-face\"\n",
        "    WEIGHTS = \"/content/yolov5s-face.pt\"\n",
        "    if not os.path.exists(YOLO_DIR):\n",
        "        # Repo includes utilities for face landmarks + NMS\n",
        "        !git clone -q https://github.com/deepcam-cn/yolov5-face.git {YOLO_DIR}\n",
        "    sys.path.append(YOLO_DIR)\n",
        "\n",
        "    # Download weights (primary + mirror)\n",
        "    if not os.path.exists(WEIGHTS):\n",
        "        urls = [\n",
        "            \"https://github.com/deepcam-cn/yolov5-face/releases/download/v1.0/yolov5s-face.pt\",\n",
        "            \"https://huggingface.co/PinPong/yolov5-face/resolve/main/yolov5s-face.pt\",\n",
        "        ]\n",
        "        for u in urls:\n",
        "            try:\n",
        "                urllib.request.urlretrieve(u, WEIGHTS)\n",
        "                break\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "    import torch\n",
        "    from models.experimental import attempt_load\n",
        "    from utils.augmentations import letterbox\n",
        "    from utils.general import non_max_suppression_face, scale_coords\n",
        "    try:\n",
        "        # Some forks provide a helper for landmark scaling; if not found, we implement small helper\n",
        "        from utils.general import scale_coords_landmarks\n",
        "    except Exception:\n",
        "        def scale_coords_landmarks(img_shape, coords, original_shape, ratio_pad=None):\n",
        "            # coords: (N,10) [le_x,le_y,...,rm_y]\n",
        "            if ratio_pad is None:\n",
        "                gain = min(img_shape[0] / original_shape[0], img_shape[1] / original_shape[1])\n",
        "                pad = ((img_shape[1] - original_shape[1] * gain) / 2, (img_shape[0] - original_shape[0] * gain) / 2)\n",
        "            else:\n",
        "                gain = ratio_pad[0]\n",
        "                pad = ratio_pad[1]\n",
        "            coords[:, 0::2] -= pad[0]  # x\n",
        "            coords[:, 1::2] -= pad[1]  # y\n",
        "            coords[:, 0::2] /= gain\n",
        "            coords[:, 1::2] /= gain\n",
        "            coords[:, 0::2] = coords[:, 0::2].clip(0, original_shape[1]-1)\n",
        "            coords[:, 1::2] = coords[:, 1::2].clip(0, original_shape[0]-1)\n",
        "            return coords\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model = attempt_load(WEIGHTS, map_location=device)\n",
        "    model.eval()\n",
        "    stride = int(getattr(model, 'stride', torch.tensor([32])).max())\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def yolo5_face_detect(img_bgr, conf_thres=CONF_THRES, iou_thres=IOU_THRES, img_size=IMG_SIZE):\n",
        "        \"\"\"\n",
        "        Returns list of dicts: {'bbox':[x1,y1,x2,y2], 'score':float, 'kps':np.array(5,2)}\n",
        "        \"\"\"\n",
        "        img0 = img_bgr\n",
        "        h0, w0 = img0.shape[:2]\n",
        "        # Letterbox\n",
        "        im, ratio, (dw, dh) = letterbox(img0, new_shape=img_size, stride=stride, auto=True, scaleFill=False)\n",
        "        im = im[:, :, ::-1].transpose(2, 0, 1)  # BGR->RGB, to CHW\n",
        "        im = np.ascontiguousarray(im)\n",
        "        im = torch.from_numpy(im).to(device).float() / 255.0\n",
        "        im = im.unsqueeze(0)\n",
        "\n",
        "        pred = model(im)[0]\n",
        "        # NMS specialized for face (with 5 landmarks)\n",
        "        det = non_max_suppression_face(pred, conf_thres, iou_thres)[0]  # (n, 15): box(4)+conf(1)+landmarks(10)\n",
        "        results = []\n",
        "        if det is None or len(det) == 0:\n",
        "            return results\n",
        "\n",
        "        # Rescale boxes and landmarks\n",
        "        det = det.cpu()\n",
        "        boxes = det[:, :4]\n",
        "        scores = det[:, 4]\n",
        "        lms = det[:, 5:15]  # five (x,y) pairs -> 10 cols\n",
        "\n",
        "        boxes = scale_coords(im.shape[2:], boxes, img0.shape).round()\n",
        "        lms = scale_coords_landmarks(im.shape[2:], lms.clone(), img0.shape)\n",
        "        for j in range(boxes.shape[0]):\n",
        "            x1, y1, x2, y2 = boxes[j].tolist()\n",
        "            score = float(scores[j].item())\n",
        "            k = lms[j].view(5, 2).numpy()\n",
        "            results.append({'bbox':[float(x1), float(y1), float(x2), float(y2)],\n",
        "                            'score':score, 'kps':k.astype(np.float32)})\n",
        "        return results\n",
        "\n",
        "    yolo_ok = True\n",
        "    print(\"[INFO] YOLOv5-face ready on\", device.upper())\n",
        "except Exception as e:\n",
        "    print(\"[WARN] YOLOv5-face init failed:\", e)\n",
        "    yolo_ok = False\n",
        "\n",
        "# ---------- SCRFD fallback (CPU only to avoid CUDA mismatches) ----------\n",
        "scrfd = None\n",
        "if not yolo_ok:\n",
        "    try:\n",
        "        from insightface.app import FaceAnalysis\n",
        "        scrfd = FaceAnalysis(name=\"buffalo_l\")\n",
        "        scrfd.prepare(ctx_id=-1, det_size=(640, 640))  # CPU\n",
        "        print(\"[INFO] Using SCRFD fallback (CPU, InsightFace).\")\n",
        "    except Exception as e:\n",
        "        print(\"[ERROR] SCRFD fallback init failed:\", e)\n",
        "\n",
        "def process_with_yolov5_face(img):\n",
        "    dets = yolo5_face_detect(img, conf_thres=CONF_THRES, iou_thres=IOU_THRES, img_size=IMG_SIZE)\n",
        "    if not dets:\n",
        "        return []\n",
        "    # sort by area\n",
        "    dets = sorted(dets, key=lambda d: face_area_bbox(d['bbox']), reverse=True)\n",
        "    chosen = dets if SAVE_ALL_FACES else [dets[0]]\n",
        "    outputs = []\n",
        "    for d in chosen:\n",
        "        kps = d.get('kps', None)\n",
        "        if kps is not None and np.array(kps).shape == (5,2):\n",
        "            aligned, kps_rot = rotate_align(img, kps)\n",
        "            box = bbox_from_landmarks(kps_rot, scale=MARGIN)\n",
        "            crop = enforce_aspect_and_pad(aligned, box, aspect=(ASPECT_W, ASPECT_H))\n",
        "        else:\n",
        "            x1,y1,x2,y2 = d['bbox']\n",
        "            cx, cy = (x1+x2)/2.0, (y1+y2)/2.0\n",
        "            bw, bh = (x2-x1), (y2-y1)\n",
        "            bw *= (1 + MARGIN); bh *= (1 + MARGIN*1.2)\n",
        "            box = [cx-bw/2.0, cy-bh/2.0, cx+bw/2.0, cy+bh/2.0]\n",
        "            crop = enforce_aspect_and_pad(img, box, aspect=(ASPECT_W, ASPECT_H))\n",
        "        if crop is not None and min(crop.shape[:2]) >= MIN_CROP_MIN_DIM:\n",
        "            outputs.append(cv2.resize(crop, TARGET_SIZE, interpolation=cv2.INTER_AREA))\n",
        "    return outputs\n",
        "\n",
        "def process_with_scrfd(img):\n",
        "    faces = scrfd.get(img)\n",
        "    faces = [f for f in faces if getattr(f, \"det_score\", 1.0) >= 0.5]\n",
        "    if not faces:\n",
        "        return []\n",
        "    faces = sorted(faces, key=lambda f: face_area_bbox(f.bbox.astype(float)), reverse=True)\n",
        "    chosen = faces if SAVE_ALL_FACES else [faces[0]]\n",
        "    outputs = []\n",
        "    for f in chosen:\n",
        "        kps = getattr(f, \"kps\", None)\n",
        "        if kps is not None and np.array(kps).shape == (5,2):\n",
        "            aligned, kps_rot = rotate_align(img, kps)\n",
        "            box = bbox_from_landmarks(kps_rot, scale=MARGIN)\n",
        "            crop = enforce_aspect_and_pad(aligned, box, aspect=(ASPECT_W, ASPECT_H))\n",
        "        else:\n",
        "            x1,y1,x2,y2 = f.bbox.astype(float)\n",
        "            cx, cy = (x1+x2)/2.0, (y1+y2)/2.0\n",
        "            bw, bh = (x2-x1), (y2-y1)\n",
        "            bw *= (1 + MARGIN); bh *= (1 + MARGIN*1.2)\n",
        "            box = [cx-bw/2.0, cy-bh/2.0, cx+bw/2.0, cy+bh/2.0]\n",
        "            crop = enforce_aspect_and_pad(img, box, aspect=(ASPECT_W, ASPECT_H))\n",
        "        if crop is not None and min(crop.shape[:2]) >= MIN_CROP_MIN_DIM:\n",
        "            outputs.append(cv2.resize(crop, TARGET_SIZE, interpolation=cv2.INTER_AREA))\n",
        "    return outputs\n",
        "\n",
        "# ---------- Process folder ----------\n",
        "paths = sorted(\n",
        "    [p for ext in (\"*.jpg\",\"*.jpeg\",\"*.png\",\"*.bmp\",\"*.webp\",\"*.tif\",\"*.tiff\")\n",
        "     for p in glob(os.path.join(INPUT_DIR, ext))]\n",
        ")\n",
        "\n",
        "print(f\"Found {len(paths)} images in {INPUT_DIR}\")\n",
        "saved = 0\n",
        "skipped = 0\n",
        "for p in tqdm(paths):\n",
        "    img = cv2.imread(p)\n",
        "    if img is None:\n",
        "        skipped += 1\n",
        "        continue\n",
        "    crops = []\n",
        "    if yolo_ok:\n",
        "        crops = process_with_yolov5_face(img)\n",
        "        # optionally try fallback if nothing found\n",
        "        if not crops and scrfd is not None:\n",
        "            crops = process_with_scrfd(img)\n",
        "    elif scrfd is not None:\n",
        "        crops = process_with_scrfd(img)\n",
        "\n",
        "    base = os.path.splitext(os.path.basename(p))[0]\n",
        "    if not crops:\n",
        "        skipped += 1\n",
        "        continue\n",
        "    if len(crops) == 1:\n",
        "        out_path = os.path.join(OUTPUT_DIR, f\"{base}.jpg\")\n",
        "        cv2.imwrite(out_path, crops[0]); saved += 1\n",
        "    else:\n",
        "        for i, c in enumerate(crops, start=1):\n",
        "            out_path = os.path.join(OUTPUT_DIR, f\"{base}_{i}.jpg\")\n",
        "            cv2.imwrite(out_path, c); saved += 1\n",
        "\n",
        "print(f\"Done.\\nSaved crops: {saved}\\nImages with no accepted face: {skipped}\\nOutput: {OUTPUT_DIR}\")\n",
        "\n",
        "# ---------- Preview ----------\n",
        "import matplotlib.pyplot as plt\n",
        "sample_outs = sorted(glob(os.path.join(OUTPUT_DIR, \"*.jpg\")))[:8]\n",
        "if sample_outs:\n",
        "    cols = 4\n",
        "    rows = int(np.ceil(len(sample_outs)/cols))\n",
        "    plt.figure(figsize=(cols*3, rows*3))\n",
        "    for i, p in enumerate(sample_outs, 1):\n",
        "        im = cv2.cvtColor(cv2.imread(p), cv2.COLOR_BGR2RGB)\n",
        "        plt.subplot(rows, cols, i)\n",
        "        plt.imshow(im); plt.title(os.path.basename(p), fontsize=8); plt.axis('off')\n",
        "    plt.tight_layout(); plt.show()\n",
        "else:\n",
        "    print(\"No output previews available yet.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_GWEfRepPE-L",
        "outputId": "e4beb908-914b-437e-d0ee-899ba086ccb1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[WARN] YOLOv5-face init failed: No module named 'utils.augmentations'\n",
            "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
            "find model: /root/.insightface/models/buffalo_l/1k3d68.onnx landmark_3d_68 ['None', 3, 192, 192] 0.0 1.0\n",
            "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
            "find model: /root/.insightface/models/buffalo_l/2d106det.onnx landmark_2d_106 ['None', 3, 192, 192] 0.0 1.0\n",
            "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
            "find model: /root/.insightface/models/buffalo_l/det_10g.onnx detection [1, 3, '?', '?'] 127.5 128.0\n",
            "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
            "find model: /root/.insightface/models/buffalo_l/genderage.onnx genderage ['None', 3, 96, 96] 0.0 1.0\n",
            "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
            "find model: /root/.insightface/models/buffalo_l/w600k_r50.onnx recognition ['None', 3, 112, 112] 127.5 127.5\n",
            "set det-size: (640, 640)\n",
            "[INFO] Using SCRFD fallback (CPU, InsightFace).\n",
            "Found 10 images in /content/images\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:07<00:00,  1.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done.\n",
            "Saved crops: 9\n",
            "Images with no accepted face: 1\n",
            "Output: /content/cropped_faces\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uQeR2zBzPFkx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}