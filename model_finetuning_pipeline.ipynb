{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dc1435",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install diffusers transformers accelerate datasets bitsandbytes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa30f7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionXLPipeline\n",
    "from peft import LoraConfig, get_peft_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ec6bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load SDXL\n",
    "pipe = StableDiffusionXLPipeline.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\")\n",
    "\n",
    "# Apply LoRA\n",
    "lora_config = LoraConfig(r=4, lora_alpha=16, target_modules=[\"to_q\", \"to_v\"])\n",
    "pipe.unet = get_peft_model(pipe.unet, lora_config)\n",
    "\n",
    "# Training loop (pseudo-code)\n",
    "for batch in dataloader:\n",
    "    text, image = batch\n",
    "    loss = pipe.train_step(text, image)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468be2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.load_lora_weights(\"path/to/lora\")\n",
    "image = pipe(\"young male, short curly hair, medium-dark skin, oval face\").images[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05172b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Train SDXL (Stable Diffusion XL 1.0-base) with LoRA on a text–image dataset.\n",
    "\n",
    "Requirements:\n",
    "    pip install torch torchvision pillow diffusers transformers accelerate datasets safetensors\n",
    "\n",
    "Data format (simple):\n",
    "    data_dir/\n",
    "      images/\n",
    "        000001.jpg\n",
    "        000002.jpg\n",
    "        ...\n",
    "      prompts.jsonl    # one JSON per line: {\"image\": \"000001.jpg\", \"prompt\": \"compressed face description ...\"}\n",
    "\n",
    "Usage (example):\n",
    "    python train_sdxl_lora.py \\\n",
    "        --data_dir /path/to/data \\\n",
    "        --output_dir ./sdxl_lora_out \\\n",
    "        --resolution 512 \\\n",
    "        --train_batch_size 4 \\\n",
    "        --gradient_accumulation_steps 2 \\\n",
    "        --learning_rate 5e-5 \\\n",
    "        --max_train_steps 5000 \\\n",
    "        --checkpointing_steps 500 \\\n",
    "        --seed 42\n",
    "\n",
    "Notes:\n",
    "- This script follows a standard diffusion training loop for SDXL and applies LoRA to UNet attention processors.\n",
    "- Prompts should already be compressed to <=77 tokens (CLIP limit) for consistency.\n",
    "- Saves LoRA weights only; load them at inference time with the SDXL pipeline.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import math\n",
    "import json\n",
    "import random\n",
    "import argparse\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "from accelerate import Accelerator\n",
    "from accelerate.utils import set_seed\n",
    "\n",
    "from diffusers import (\n",
    "    StableDiffusionXLPipeline,\n",
    "    AutoencoderKL,\n",
    "    DDIMScheduler,\n",
    ")\n",
    "from diffusers.utils import logging\n",
    "from diffusers.models.attention_processor import LoRAAttnProcessor, LoRAAttnProcessor2_0\n",
    "from transformers import AutoTokenizer, CLIPTextModel, CLIPTextModelWithProjection\n",
    "\n",
    "from safetensors.torch import save_file\n",
    "\n",
    "logger = logging.get_logger(__name__)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Dataset\n",
    "# -------------------------\n",
    "\n",
    "class TextImageDataset(Dataset):\n",
    "    def __init__(self, data_dir: str, resolution: int):\n",
    "        self.images_dir = os.path.join(data_dir, \"images\")\n",
    "        prompts_path = os.path.join(data_dir, \"prompts.jsonl\")\n",
    "        assert os.path.exists(self.images_dir), f\"Missing images dir: {self.images_dir}\"\n",
    "        assert os.path.exists(prompts_path), f\"Missing prompts.jsonl: {prompts_path}\"\n",
    "\n",
    "        self.items: List[Dict[str, Any]] = []\n",
    "        with open(prompts_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                obj = json.loads(line)\n",
    "                img_name = obj[\"image\"]\n",
    "                prompt = obj[\"prompt\"]\n",
    "                img_path = os.path.join(self.images_dir, img_name)\n",
    "                if os.path.exists(img_path):\n",
    "                    self.items.append({\"image\": img_path, \"prompt\": prompt})\n",
    "\n",
    "        assert len(self.items) > 0, \"No valid items found.\"\n",
    "\n",
    "        self.transform = transforms.Compose([\n",
    "            CenterSquareCrop(),\n",
    "            transforms.Resize((resolution, resolution), interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5], [0.5]),\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        item = self.items[idx]\n",
    "        image = Image.open(item[\"image\"]).convert(\"RGB\")\n",
    "        image = self.transform(image)\n",
    "        prompt = item[\"prompt\"]\n",
    "        return {\"pixel_values\": image, \"prompt\": prompt}\n",
    "\n",
    "\n",
    "class CenterSquareCrop:\n",
    "    \"\"\"Center-crop to square by the shorter side.\"\"\"\n",
    "    def __call__(self, img: Image.Image) -> Image.Image:\n",
    "        w, h = img.size\n",
    "        side = min(w, h)\n",
    "        left = (w - side) // 2\n",
    "        top = (h - side) // 2\n",
    "        return img.crop((left, top, left + side, top + side))\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# LoRA utilities\n",
    "# -------------------------\n",
    "\n",
    "def is_torch2_available():\n",
    "    return hasattr(torch, \"__version__\") and int(torch.__version__.split(\".\")[0]) >= 2\n",
    "\n",
    "def inject_lora_unet(unet, r: int = 4):\n",
    "    \"\"\"\n",
    "    Replace attention processors with LoRA-enabled ones.\n",
    "    \"\"\"\n",
    "    lora_cls = LoRAAttnProcessor2_0 if is_torch2_available() else LoRAAttnProcessor\n",
    "    for name, module in unet.named_modules():\n",
    "        if hasattr(module, \"set_processor\"):\n",
    "            # create LoRA processor with rank r\n",
    "            # NOTE: processor knows in/out dims internally\n",
    "            module.set_processor(lora_cls(r=r))\n",
    "    return unet\n",
    "\n",
    "def collect_lora_parameters(unet):\n",
    "    \"\"\"\n",
    "    Return only LoRA parameters for optimization/saving.\n",
    "    \"\"\"\n",
    "    lora_params = []\n",
    "    for name, module in unet.named_modules():\n",
    "        proc = getattr(module, \"processor\", None)\n",
    "        if proc is None:\n",
    "            continue\n",
    "        # LoRA processors store weights in 'lora_*' attributes\n",
    "        params = [p for p in module.parameters() if p.requires_grad]\n",
    "        if params:\n",
    "            lora_params.extend(params)\n",
    "    return lora_params\n",
    "\n",
    "def save_unet_lora(unet, save_path: str):\n",
    "    \"\"\"\n",
    "    Save LoRA weights from UNet to a safetensors file.\n",
    "    \"\"\"\n",
    "    state = {}\n",
    "    for name, module in unet.named_modules():\n",
    "        proc = getattr(module, \"processor\", None)\n",
    "        if proc is None:\n",
    "            continue\n",
    "        # grab parameters that require grad (LoRA params)\n",
    "        for pname, param in module.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                key = f\"{name}.{pname}\"\n",
    "                state[key] = param.detach().cpu()\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    save_file(state, save_path)\n",
    "    logger.info(f\"Saved UNet LoRA weights to: {save_path}\")\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# SDXL prompt encoding\n",
    "# -------------------------\n",
    "\n",
    "@dataclass\n",
    "class PromptBatch:\n",
    "    prompt: List[str]\n",
    "    negative_prompt: List[str]\n",
    "\n",
    "\n",
    "def encode_sdxl_prompts(pipe: StableDiffusionXLPipeline, prompts: List[str], device: torch.device):\n",
    "    \"\"\"\n",
    "    Encode prompts for SDXL's dual text encoders (CLIP-ViT-L and OpenCLIP ViT-bigG).\n",
    "    Returns a dict of embeddings consumed by UNet forward.\n",
    "    \"\"\"\n",
    "    # SDXL helper handles dual encoders internally\n",
    "    # Use blank negative or configurable one\n",
    "    negs = [\"\"] * len(prompts)\n",
    "    enc = pipe.encode_prompt(\n",
    "        prompt=prompts,\n",
    "        negative_prompt=negs,\n",
    "        device=device,\n",
    "        num_images_per_prompt=1,\n",
    "        do_classifier_free_guidance=True,\n",
    "    )\n",
    "    return enc\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Training\n",
    "# -------------------------\n",
    "\n",
    "def train(args):\n",
    "    accelerator = Accelerator(mixed_precision=args.mixed_precision)\n",
    "    set_seed(args.seed)\n",
    "\n",
    "    # Load base SDXL pipeline\n",
    "    pipe = StableDiffusionXLPipeline.from_pretrained(\n",
    "        \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "        torch_dtype=torch.float16 if accelerator.mixed_precision == \"fp16\" else torch.float32,\n",
    "        use_safetensors=True,\n",
    "    )\n",
    "    pipe.set_progress_bar_config(disable=True)\n",
    "    pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)  # training noise scheduling\n",
    "\n",
    "    # Freeze VAE and text encoders; train LoRA on UNet only\n",
    "    vae: AutoencoderKL = pipe.vae\n",
    "    text_encoder: CLIPTextModel = pipe.text_encoder\n",
    "    text_encoder_2: CLIPTextModelWithProjection = pipe.text_encoder_2\n",
    "    unet = pipe.unet\n",
    "\n",
    "    vae.requires_grad_(False)\n",
    "    text_encoder.requires_grad_(False)\n",
    "    text_encoder_2.requires_grad_(False)\n",
    "\n",
    "    # Inject LoRA into UNet attention processors\n",
    "    inject_lora_unet(unet, r=args.lora_rank)\n",
    "    lora_params = collect_lora_parameters(unet)\n",
    "    assert len(lora_params) > 0, \"No LoRA parameters found in UNet.\"\n",
    "\n",
    "    # Dataset / Dataloader\n",
    "    dataset = TextImageDataset(args.data_dir, resolution=args.resolution)\n",
    "    dl = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=args.train_batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=args.num_workers,\n",
    "        drop_last=True,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.AdamW(lora_params, lr=args.learning_rate)\n",
    "\n",
    "    # Prepare with accelerator\n",
    "    unet, optimizer, dl = accelerator.prepare(unet, optimizer, dl)\n",
    "    vae.to(accelerator.device)\n",
    "    text_encoder.to(accelerator.device)\n",
    "    text_encoder_2.to(accelerator.device)\n",
    "\n",
    "    # Latent scaling constant used by SDXL/SD pipelines\n",
    "    vae_scale_factor = 0.18215\n",
    "\n",
    "    # Training state\n",
    "    global_step = 0\n",
    "    num_update_steps_per_epoch = math.ceil(len(dl) / args.gradient_accumulation_steps)\n",
    "    max_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n",
    "\n",
    "    # Optionally resume…\n",
    "    os.makedirs(args.output_dir, exist_ok=True)\n",
    "\n",
    "    accelerator.print(f\"Starting training: steps={args.max_train_steps}, epochs={max_epochs}\")\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        unet.train()\n",
    "\n",
    "        for step, batch in enumerate(dl):\n",
    "            with accelerator.accumulate(unet):\n",
    "                # Images → latents\n",
    "                pixel_values = batch[\"pixel_values\"].to(accelerator.device)\n",
    "                latents = vae.encode(pixel_values).latent_dist.sample()\n",
    "                latents = latents * vae_scale_factor\n",
    "\n",
    "                # Sample timesteps and add noise\n",
    "                bsz = latents.shape[0]\n",
    "                timesteps = torch.randint(0, pipe.scheduler.config.num_train_timesteps, (bsz,), device=accelerator.device).long()\n",
    "                noise = torch.randn_like(latents)\n",
    "                noisy_latents = pipe.scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "                # Encode prompts (dual encoders handled internally)\n",
    "                enc = encode_sdxl_prompts(pipe, batch[\"prompt\"], accelerator.device)\n",
    "\n",
    "                # UNet forward\n",
    "                model_pred = unet(\n",
    "                    noisy_latents,\n",
    "                    timesteps,\n",
    "                    prompt_embeds=enc[\"prompt_embeds\"],\n",
    "                    pooled_prompt_embeds=enc[\"pooled_prompt_embeds\"],\n",
    "                    add_text_embeds=enc.get(\"add_text_embeds\", None),\n",
    "                    add_time_ids=enc.get(\"add_time_ids\", None),\n",
    "                ).sample\n",
    "\n",
    "                # Standard diffusion training: predict noise\n",
    "                loss = F.mse_loss(model_pred, noise, reduction=\"mean\")\n",
    "\n",
    "                accelerator.backward(loss)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            if accelerator.sync_gradients:\n",
    "                global_step += 1\n",
    "\n",
    "                # Checkpointing\n",
    "                if args.checkpointing_steps > 0 and global_step % args.checkpointing_steps == 0:\n",
    "                    # Save LoRA weights only\n",
    "                    lora_path = os.path.join(args.output_dir, f\"unet_lora_step_{global_step}.safetensors\")\n",
    "                    save_unet_lora(unet, lora_path)\n",
    "                    accelerator.print(f\"[epoch {epoch}] step {global_step} | loss={loss.item():.4f} | saved {lora_path}\")\n",
    "\n",
    "            # Stop condition\n",
    "            if global_step >= args.max_train_steps:\n",
    "                break\n",
    "\n",
    "        accelerator.print(f\"Epoch {epoch+1}/{max_epochs} completed. Last loss: {loss.item():.4f}\")\n",
    "        if global_step >= args.max_train_steps:\n",
    "            break\n",
    "\n",
    "    # Final save\n",
    "    final_lora_path = os.path.join(args.output_dir, \"unet_lora_final.safetensors\")\n",
    "    save_unet_lora(unet, final_lora_path)\n",
    "    accelerator.print(f\"Training complete. Final LoRA saved to {final_lora_path}\")\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Inference helper (optional)\n",
    "# -------------------------\n",
    "\n",
    "def load_pipe_with_lora(lora_path: str, fp16: bool = True) -> StableDiffusionXLPipeline:\n",
    "    dtype = torch.float16 if fp16 and torch.cuda.is_available() else torch.float32\n",
    "    pipe = StableDiffusionXLPipeline.from_pretrained(\n",
    "        \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "        torch_dtype=dtype,\n",
    "        use_safetensors=True,\n",
    "    )\n",
    "    # Re-inject LoRA processors and load weights\n",
    "    inject_lora_unet(pipe.unet, r=4)\n",
    "    # Load weights into matching parameter names\n",
    "    state = torch.load(lora_path, map_location=\"cpu\") if lora_path.endswith(\".pt\") else None\n",
    "    if state is None:\n",
    "        from safetensors import safe_open\n",
    "        tensors = {}\n",
    "        with safe_open(lora_path, framework=\"pt\", device=\"cpu\") as f:\n",
    "            for k in f.keys():\n",
    "                tensors[k] = f.get_tensor(k)\n",
    "        state = tensors\n",
    "    missing = pipe.unet.load_state_dict(state, strict=False)\n",
    "    print(\"Loaded LoRA; missing keys (expected for non-LoRA params):\", missing.missing_keys)\n",
    "    pipe = pipe.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    pipe.set_progress_bar_config(disable=False)\n",
    "    return pipe\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# CLI\n",
    "# -------------------------\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=\"Train SDXL with LoRA on text-image pairs.\")\n",
    "    parser.add_argument(\"--data_dir\", type=str, required=True, help=\"Directory containing images/ and prompts.jsonl\")\n",
    "    parser.add_argument(\"--output_dir\", type=str, required=True, help=\"Directory to save LoRA checkpoints\")\n",
    "    parser.add_argument(\"--resolution\", type=int, default=512, help=\"Input resolution (SDXL trained on 1024; we use 512)\")\n",
    "    parser.add_argument(\"--train_batch_size\", type=int, default=4)\n",
    "    parser.add_argument(\"--gradient_accumulation_steps\", type=int, default=2)\n",
    "    parser.add_argument(\"--max_train_steps\", type=int, default=5000)\n",
    "    parser.add_argument(\"--checkpointing_steps\", type=int, default=500)\n",
    "    parser.add_argument(\"--learning_rate\", type=float, default=5e-5)\n",
    "    parser.add_argument(\"--mixed_precision\", type=str, default=\"fp16\", choices=[\"no\", \"fp16\", \"bf16\"])\n",
    "    parser.add_argument(\"--num_workers\", type=int, default=4)\n",
    "    parser.add_argument(\"--lora_rank\", type=int, default=4, help=\"LoRA rank for attention processors\")\n",
    "    parser.add_argument(\"--seed\", type=int, default=42)\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = parse_args()\n",
    "    train(args)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
