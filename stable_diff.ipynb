{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ffcd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ----------------------------\n",
    "# Text encoder (placeholder)\n",
    "# ----------------------------\n",
    "class TinyTextEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size=10000, dim=768, n_layers=4, n_heads=12, max_len=77):\n",
    "        super().__init__()\n",
    "        self.token = nn.Embedding(vocab_size, dim)\n",
    "        self.pos = nn.Embedding(max_len, dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=dim, nhead=n_heads, dim_feedforward=dim*4, batch_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        self.dim = dim\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def forward(self, token_ids):\n",
    "        # token_ids: (B, L)\n",
    "        B, L = token_ids.shape\n",
    "        pos_ids = torch.arange(L, device=token_ids.device).unsqueeze(0).expand(B, L)\n",
    "        x = self.token(token_ids) + self.pos(pos_ids)\n",
    "        x = self.encoder(x)  # (B, L, dim)\n",
    "        return x  # sequence of text embeddings for cross-attention\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# VAE for latent space\n",
    "# ----------------------------\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, downsample=True):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
    "        self.norm1 = nn.GroupNorm(8, out_ch)\n",
    "        self.norm2 = nn.GroupNorm(8, out_ch)\n",
    "        self.down = nn.Conv2d(out_ch, out_ch, 3, stride=2, padding=1) if downsample else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.silu(self.norm1(self.conv1(x)))\n",
    "        x = F.silu(self.norm2(self.conv2(x)))\n",
    "        x = self.down(x)\n",
    "        return x\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, upsample=True):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
    "        self.norm1 = nn.GroupNorm(8, out_ch)\n",
    "        self.norm2 = nn.GroupNorm(8, out_ch)\n",
    "        self.up = nn.ConvTranspose2d(out_ch, out_ch, 4, stride=2, padding=1) if upsample else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.silu(self.norm1(self.conv1(x)))\n",
    "        x = F.silu(self.norm2(self.conv2(x)))\n",
    "        x = self.up(x)\n",
    "        return x\n",
    "\n",
    "class SimpleVAE(nn.Module):\n",
    "    def __init__(self, in_ch=3, latent_ch=4):\n",
    "        super().__init__()\n",
    "        # Encoder to latent\n",
    "        self.enc1 = EncoderBlock(in_ch, 64)\n",
    "        self.enc2 = EncoderBlock(64, 128)\n",
    "        self.enc3 = EncoderBlock(128, 256)\n",
    "        self.enc4 = EncoderBlock(256, 256, downsample=False)\n",
    "        self.to_mu = nn.Conv2d(256, latent_ch, 1)\n",
    "        self.to_logvar = nn.Conv2d(256, latent_ch, 1)\n",
    "        # Decoder from latent\n",
    "        self.dec1 = DecoderBlock(latent_ch, 256, upsample=False)\n",
    "        self.dec2 = DecoderBlock(256, 256)\n",
    "        self.dec3 = DecoderBlock(256, 128)\n",
    "        self.dec4 = DecoderBlock(128, 64)\n",
    "        self.to_img = nn.Conv2d(64, in_ch, 1)\n",
    "        self.scaling = 0.18215  # typical SD latent scaling\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.enc4(self.enc3(self.enc2(self.enc1(x))))\n",
    "        mu, logvar = self.to_mu(h), self.to_logvar(h)\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        z = mu + std * torch.randn_like(std)\n",
    "        return z * self.scaling, mu, logvar\n",
    "\n",
    "    def decode(self, z):\n",
    "        z = z / self.scaling\n",
    "        h = self.dec4(self.dec3(self.dec2(self.dec1(z))))\n",
    "        x_rec = self.to_img(h)\n",
    "        return x_rec\n",
    "\n",
    "    def forward(self, x):\n",
    "        z, mu, logvar = self.encode(x)\n",
    "        x_rec = self.decode(z)\n",
    "        return x_rec, mu, logvar\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Cross-attention block\n",
    "# ----------------------------\n",
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, dim_q, dim_kv, n_heads=8):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.to_q = nn.Linear(dim_q, dim_q)\n",
    "        self.to_k = nn.Linear(dim_kv, dim_q)\n",
    "        self.to_v = nn.Linear(dim_kv, dim_q)\n",
    "        self.proj = nn.Linear(dim_q, dim_q)\n",
    "\n",
    "    def forward(self, q, kv):\n",
    "        # q: (B, Nq, Dq) image features as sequence\n",
    "        # kv: (B, Nk, Dkv) text embeddings\n",
    "        B, Nq, Dq = q.shape\n",
    "        Nk = kv.shape[1]\n",
    "        qh = self.to_q(q).view(B, Nq, self.n_heads, Dq // self.n_heads).transpose(1, 2)  # (B,H,Nq,Dh)\n",
    "        kh = self.to_k(kv).view(B, Nk, self.n_heads, Dq // self.n_heads).transpose(1, 2) # (B,H,Nk,Dh)\n",
    "        vh = self.to_v(kv).view(B, Nk, self.n_heads, Dq // self.n_heads).transpose(1, 2) # (B,H,Nk,Dh)\n",
    "        attn = torch.softmax((qh @ kh.transpose(-2, -1)) / (Dq // self.n_heads) ** 0.5, dim=-1)  # (B,H,Nq,Nk)\n",
    "        out = attn @ vh  # (B,H,Nq,Dh)\n",
    "        out = out.transpose(1, 2).contiguous().view(B, Nq, Dq)\n",
    "        return self.proj(out)\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# UNet block with cross-attention\n",
    "# ----------------------------\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, ch):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(ch, ch, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(ch, ch, 3, padding=1)\n",
    "        self.norm1 = nn.GroupNorm(8, ch)\n",
    "        self.norm2 = nn.GroupNorm(8, ch)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = F.silu(self.norm1(self.conv1(x)))\n",
    "        h = self.norm2(self.conv2(h))\n",
    "        return F.silu(h + x)\n",
    "\n",
    "class UNetWithText(nn.Module):\n",
    "    def __init__(self, in_ch=4, base_ch=256, text_dim=768):\n",
    "        super().__init__()\n",
    "        # Down path\n",
    "        self.in_conv = nn.Conv2d(in_ch, base_ch, 3, padding=1)\n",
    "        self.down1 = nn.Conv2d(base_ch, base_ch, 3, stride=2, padding=1)\n",
    "        self.rb1 = ResBlock(base_ch)\n",
    "        self.down2 = nn.Conv2d(base_ch, base_ch, 3, stride=2, padding=1)\n",
    "        self.rb2 = ResBlock(base_ch)\n",
    "\n",
    "        # Bottleneck + cross-attention\n",
    "        self.rb_mid = ResBlock(base_ch)\n",
    "        self.txt_proj = nn.Linear(text_dim, base_ch)\n",
    "        self.cross = CrossAttention(dim_q=base_ch, dim_kv=text_dim, n_heads=8)\n",
    "\n",
    "        # Up path\n",
    "        self.up1 = nn.ConvTranspose2d(base_ch, base_ch, 4, stride=2, padding=1)\n",
    "        self.rb3 = ResBlock(base_ch)\n",
    "        self.up2 = nn.ConvTranspose2d(base_ch, base_ch, 4, stride=2, padding=1)\n",
    "        self.rb4 = ResBlock(base_ch)\n",
    "\n",
    "        # Output head (predict epsilon in latent space)\n",
    "        self.out = nn.Conv2d(base_ch, in_ch, 3, padding=1)\n",
    "\n",
    "        # Timestep embedding\n",
    "        self.t_embed = nn.Sequential(\n",
    "            nn.Linear(1, base_ch), nn.SiLU(), nn.Linear(base_ch, base_ch)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_t, t, text_emb):\n",
    "        # x_t: (B,4,H,W), t: (B,), text_emb: (B,L,768)\n",
    "        B, _, H, W = x_t.shape\n",
    "        h = F.silu(self.in_conv(x_t))\n",
    "        h = self.down1(h); h = self.rb1(h)\n",
    "        h = self.down2(h); h = self.rb2(h)\n",
    "\n",
    "        # Add timestep conditioning (FiLM-like bias)\n",
    "        t_inp = t.view(B, 1)\n",
    "        gamma = self.t_embed(t_inp).view(B, -1, 1, 1)\n",
    "        h = h + gamma\n",
    "\n",
    "        # Bottleneck + cross-attention: flatten spatial to sequence\n",
    "        h = self.rb_mid(h)\n",
    "        seq = h.view(B, -1, H//4 * W//4).transpose(1, 2)  # (B, Nq, C)\n",
    "        seq = seq + self.cross(seq, text_emb)             # conditioned on text\n",
    "        h = seq.transpose(1, 2).view(B, -1, H//4, W//4)\n",
    "\n",
    "        # Up path\n",
    "        h = self.up1(h); h = self.rb3(h)\n",
    "        h = self.up2(h); h = self.rb4(h)\n",
    "\n",
    "        eps = self.out(h)  # predicted noise in latent space\n",
    "        return eps\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Scheduler helper (DDPM-like)\n",
    "# ----------------------------\n",
    "class SimpleDDPMSchedule:\n",
    "    def __init__(self, timesteps=1000):\n",
    "        betas = torch.linspace(1e-4, 0.02, timesteps)\n",
    "        alphas = 1.0 - betas\n",
    "        self.alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
    "        self.timesteps = timesteps\n",
    "\n",
    "    def get_alpha_sigma(self, t_cont):\n",
    "        # t_cont in [0,1], map to discrete index\n",
    "        idx = (t_cont * (self.timesteps - 1)).long().clamp(0, self.timesteps - 1)\n",
    "        alpha = torch.sqrt(self.alphas_cumprod[idx])\n",
    "        sigma = torch.sqrt(1.0 - self.alphas_cumprod[idx])\n",
    "        return alpha.view(-1, 1, 1, 1), sigma.view(-1, 1, 1, 1)\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Training step (noise prediction loss with text conditioning)\n",
    "# ----------------------------\n",
    "def training_step(vae, unet, schedule, images, token_ids):\n",
    "    # Encode images to latents\n",
    "    latents, mu, logvar = vae.encode(images)  # (B,4,H/8,W/8) roughly\n",
    "\n",
    "    # Sample timesteps and noise\n",
    "    B = latents.size(0)\n",
    "    t = torch.rand(B, device=latents.device)\n",
    "    alpha, sigma = schedule.get_alpha_sigma(t)\n",
    "    noise = torch.randn_like(latents)\n",
    "    x_t = alpha * latents + sigma * noise\n",
    "\n",
    "    # Encode text\n",
    "    text_encoder = TinyTextEncoder()\n",
    "    text_encoder.to(images.device)\n",
    "    text_emb = text_encoder(token_ids)  # (B,L,768)\n",
    "\n",
    "    # Predict noise\n",
    "    eps_pred = unet(x_t, t, text_emb)\n",
    "\n",
    "    # Loss: MSE between true noise and predicted noise (Îµ-prediction)\n",
    "    loss = F.mse_loss(eps_pred, noise)\n",
    "\n",
    "    # VAE KL term (optional, if training VAE jointly)\n",
    "    kl = (-0.5 * (1 + logvar - mu.pow(2) - logvar.exp())).mean()\n",
    "    return loss + 1e-4 * kl\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Inference (classifier-free guidance, minimal)\n",
    "# ----------------------------\n",
    "@torch.no_grad()\n",
    "def sample_images(vae, unet, schedule, token_ids, steps=50, guidance_scale=7.5, shape=(4, 64, 64)):\n",
    "    device = next(unet.parameters()).device\n",
    "    B = token_ids.size(0)\n",
    "    x = torch.randn((B, *shape), device=device)\n",
    "\n",
    "    # Build text encoder once\n",
    "    text_encoder = TinyTextEncoder().to(device)\n",
    "    text_emb = text_encoder(token_ids)\n",
    "    empty_tokens = torch.zeros_like(token_ids)  # simplistic \"empty\" prompt\n",
    "    empty_emb = text_encoder(empty_tokens)\n",
    "\n",
    "    ts = torch.linspace(1.0, 0.0, steps+1, device=device)\n",
    "    for i in range(steps):\n",
    "        t = ts[i].expand(B)\n",
    "        alpha, sigma = schedule.get_alpha_sigma(t)\n",
    "        # Predict eps (conditional & unconditional)\n",
    "        eps_uncond = unet(x, t, empty_emb)\n",
    "        eps_cond = unet(x, t, text_emb)\n",
    "        eps_guided = eps_uncond + guidance_scale * (eps_cond - eps_uncond)\n",
    "        # DDIM-like update (simplified)\n",
    "        x0_pred = (x - sigma * eps_guided) / (alpha + 1e-8)\n",
    "        if i < steps - 1:\n",
    "            t_next = ts[i+1].expand(B)\n",
    "            alpha_next, sigma_next = schedule.get_alpha_sigma(t_next)\n",
    "            x = alpha_next * x0_pred + sigma_next * torch.randn_like(x)\n",
    "        else:\n",
    "            x = x0_pred\n",
    "\n",
    "    # Decode latents to images\n",
    "    imgs = vae.decode(x)\n",
    "    return imgs.clamp(-1, 1)\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Example wiring (toy run)\n",
    "# ----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    vae = SimpleVAE(in_ch=3, latent_ch=4).to(device)\n",
    "    unet = UNetWithText(in_ch=4, base_ch=256, text_dim=768).to(device)\n",
    "    schedule = SimpleDDPMSchedule(timesteps=1000)\n",
    "\n",
    "    # Dummy batch: images in [-1,1], tokens as ints\n",
    "    images = torch.randn(4, 3, 256, 256, device=device)\n",
    "    token_ids = torch.randint(0, 10000, (4, 77), device=device)\n",
    "\n",
    "    # One training step\n",
    "    loss = training_step(vae, unet, schedule, images, token_ids)\n",
    "    print(\"Training loss:\", float(loss))\n",
    "\n",
    "    # One sampling run\n",
    "    imgs = sample_images(vae, unet, schedule, token_ids, steps=10, guidance_scale=5.0, shape=(4, 32, 32))\n",
    "    print(\"Generated images shape:\", imgs.shape)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
