{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4134062c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 5. Training Parameters (Aligned with Report) ---\n",
    "print(\"\\n--- Defining LoRA Training Parameters ---\")\n",
    "pretrained_model = \"runwayml/stable-diffusion-v1-5\"\n",
    "output_dir = \"lora_face_model\"\n",
    "\n",
    "\n",
    "# --- LoRA Hyperparameters (Report Aligned) ---\n",
    "training_config = {\n",
    "    \"model_id\": \"runwayml/stable-diffusion-v1-5\", #\n",
    "    \"dataset_path\": \"/content/lora_dataset\",      # [cite: 219]\n",
    "    \"output_name\": \"lora_face_model\",\n",
    "    \"resolution\": 512,                            #\n",
    "    \"batch_size\": 1,                              # Optimized for memory\n",
    "    \"gradient_accumulation\": 4,                   # Effective batch of 4\n",
    "    \"learning_rate\": 1e-4,                        # Typical for LoRA [cite: 434]\n",
    "    \"max_steps\": 1000,                            # [cite: 220]\n",
    "    \"precision\": \"fp16\",                          # Essential for 8GB VRAM\n",
    "    \"checkpoint_freq\": 500,\n",
    "    \"seed\": 42\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# --- Build the Acceleration Command ---\n",
    "accelerate_command = (\n",
    "    f\"accelerate launch train_text_to_image_lora.py \"\n",
    "    f\"--pretrained_model_name_or_path='{training_config['model_id']}' \"\n",
    "    f\"--train_data_dir='{training_config['dataset_path']}' \"\n",
    "    f\"--caption_column='text' \"\n",
    "    f\"--resolution={training_config['resolution']} \"\n",
    "    f\"--random_flip \"\n",
    "    f\"--train_batch_size={training_config['batch_size']} \"\n",
    "    f\"--gradient_accumulation_steps={training_config['gradient_accumulation']} \"\n",
    "    f\"--gradient_checkpointing \"           # Critical for 8GB VRAM\n",
    "    f\"--use_8bit_adam \"                    # Required for memory efficiency\n",
    "    f\"--max_train_steps={training_config['max_steps']} \"\n",
    "    f\"--learning_rate={training_config['learning_rate']} \"\n",
    "    f\"--lr_scheduler='constant' \"\n",
    "    f\"--lr_warmup_steps=0 \"\n",
    "    f\"--seed={training_config['seed']} \"\n",
    "    f\"--output_dir='{training_config['output_name']}' \"\n",
    "    f\"--mixed_precision='{training_config['precision']}' \"\n",
    "    f\"--enable_xformers_memory_efficient_attention \"\n",
    "    f\"--checkpointing_steps={training_config['checkpoint_freq']} \"\n",
    ")\n",
    "\n",
    "print(f\"--- Launching Training: {training_config['model_id']} ---\")\n",
    "!{accelerate_command}\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
