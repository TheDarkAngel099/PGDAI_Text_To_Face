{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f25947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Setup ---\n",
    "!pip install diffusers transformers accelerate safetensors torch torchvision pillow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f56064",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, time, math, json\n",
    "import torch, torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from accelerate import Accelerator\n",
    "from accelerate.utils import set_seed\n",
    "from diffusers import StableDiffusionXLPipeline, AutoencoderKL, DDIMScheduler\n",
    "from diffusers.models.attention_processor import LoRAAttnProcessor, LoRAAttnProcessor2_0\n",
    "from safetensors.torch import save_file\n",
    "from transformers import CLIPTokenizer, CLIPTextModel\n",
    "\n",
    "# --- Dataset ---\n",
    "class TextImageDataset(Dataset):\n",
    "    def __init__(self, data_dir, resolution=512):\n",
    "        self.images_dir = os.path.join(data_dir, \"images\")\n",
    "        self.prompts_path = os.path.join(data_dir, \"prompts.json\")\n",
    "        self.items = [json.loads(line) for line in open(self.prompts_path)]\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((resolution, resolution), interpolation=Image.BICUBIC),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5], [0.5]),\n",
    "        ])\n",
    "    def __len__(self): return len(self.items)\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.items[idx]\n",
    "        img = Image.open(os.path.join(self.images_dir, item[\"image\"])).convert(\"RGB\")\n",
    "        return {\"pixel_values\": self.transform(img), \"prompt\": item[\"prompt\"]}\n",
    "\n",
    "# --- LoRA helpers ---\n",
    "def inject_lora_unet(unet, r=4):\n",
    "    lora_cls = LoRAAttnProcessor2_0 if hasattr(torch, \"compile\") else LoRAAttnProcessor\n",
    "    for _, module in unet.named_modules():\n",
    "        if hasattr(module, \"set_processor\"):\n",
    "            module.set_processor(lora_cls(r=r))\n",
    "    return unet\n",
    "\n",
    "def save_unet_lora(unet, save_path):\n",
    "    state = {}\n",
    "    for name, module in unet.named_modules():\n",
    "        proc = getattr(module, \"processor\", None)\n",
    "        if proc is None: continue\n",
    "        for pname, param in module.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                state[f\"{name}.{pname}\"] = param.detach().cpu()\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    save_file(state, save_path)\n",
    "    print(f\"Saved LoRA weights to {save_path}\")\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def encode_long_prompt(prompt, device):\n",
    "    tokens = tokenizer(prompt, truncation=False, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "    # Split into chunks of 77\n",
    "    chunks = tokens[0].split(77)\n",
    "    embeddings = []\n",
    "    for chunk in chunks:\n",
    "        emb = text_encoder(chunk.unsqueeze(0))[0]\n",
    "        embeddings.append(emb)\n",
    "    # Concatenate along sequence dimension\n",
    "    final_emb = torch.cat(embeddings, dim=1)\n",
    "    return final_emb\n",
    "\n",
    "# --- Training loop ---\n",
    "data_dir = \"./data_out\"   # your preprocessed dataset\n",
    "out_dir  = \"./sdxl_lora_out\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "accelerator = Accelerator(mixed_precision=\"fp16\")\n",
    "set_seed(42)\n",
    "\n",
    "pipe = StableDiffusionXLPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "    torch_dtype=torch.float16,\n",
    "    use_safetensors=True,\n",
    ")\n",
    "pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)\n",
    "tokenizer = CLIPTokenizer.from_pretrained(pipe.text_encoder.config._name_or_path)\n",
    "text_encoder = CLIPTextModel.from_pretrained(pipe.text_encoder.config._name_or_path)\n",
    "\n",
    "vae, unet = pipe.vae, pipe.unet\n",
    "vae.requires_grad_(False)\n",
    "inject_lora_unet(unet, r=4)\n",
    "lora_params = [p for p in unet.parameters() if p.requires_grad]\n",
    "\n",
    "dataset = TextImageDataset(data_dir, resolution=512)\n",
    "dl = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "optimizer = torch.optim.AdamW(lora_params, lr=5e-5)\n",
    "unet, optimizer, dl = accelerator.prepare(unet, optimizer, dl)\n",
    "vae.to(accelerator.device)\n",
    "\n",
    "vae_scale_factor = 0.18215\n",
    "global_step, last_save = 0, time.time()\n",
    "\n",
    "for epoch in range(3):   # adjust epochs\n",
    "    for batch in dl:\n",
    "        with accelerator.accumulate(unet):\n",
    "            latents = vae.encode(batch[\"pixel_values\"].to(accelerator.device)).latent_dist.sample()\n",
    "            latents = latents * vae_scale_factor\n",
    "            noise = torch.randn_like(latents)\n",
    "            timesteps = torch.randint(0, pipe.scheduler.config.num_train_timesteps, (latents.shape[0],), device=accelerator.device).long()\n",
    "            noisy_latents = pipe.scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "            prompt_embeds = encode_long_prompt(batch[\"prompt\"], accelerator.device)\n",
    "            model_pred = unet(noisy_latents, timesteps,\n",
    "                                 prompt_embeds=prompt_embeds).sample\n",
    "            loss = F.mse_loss(model_pred, noise)\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        global_step += 1\n",
    "        # Save every 45 minutes\n",
    "        if time.time() - last_save > 45*60:\n",
    "            save_unet_lora(unet, os.path.join(out_dir, f\"lora_step{global_step}.safetensors\"))\n",
    "            last_save = time.time()\n",
    "\n",
    "save_unet_lora(unet, os.path.join(out_dir, \"lora_final.safetensors\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2579914d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
