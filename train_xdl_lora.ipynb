{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f25947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Setup ---\n",
    "!pip install diffusers transformers accelerate safetensors torch torchvision pillow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f56064",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, time, math, json\n",
    "import torch, torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from accelerate import Accelerator\n",
    "from accelerate.utils import set_seed\n",
    "from diffusers import StableDiffusionXLPipeline, AutoencoderKL, DDIMScheduler\n",
    "from diffusers.models.attention_processor import LoRAAttnProcessor, LoRAAttnProcessor2_0\n",
    "from safetensors.torch import save_file\n",
    "from transformers import CLIPTokenizer, CLIPTextModel\n",
    "\n",
    "# --- Dataset ---\n",
    "class TextImageDataset(Dataset):\n",
    "    def __init__(self, data_dir, resolution=512):\n",
    "        self.images_dir = os.path.join(data_dir, \"images\")\n",
    "        self.prompts_path = os.path.join(data_dir, \"prompts.json\")\n",
    "        self.items = [json.loads(line) for line in open(self.prompts_path)]\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((resolution, resolution), interpolation=Image.BICUBIC),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5], [0.5]),\n",
    "        ])\n",
    "    def __len__(self): return len(self.items)\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.items[idx]\n",
    "        img = Image.open(os.path.join(self.images_dir, item[\"image\"])).convert(\"RGB\")\n",
    "        return {\"pixel_values\": self.transform(img), \"prompt\": item[\"prompt\"]}\n",
    "\n",
    "# --- LoRA helpers ---\n",
    "def inject_lora_unet(unet, r=4):\n",
    "    lora_cls = LoRAAttnProcessor2_0 if hasattr(torch, \"compile\") else LoRAAttnProcessor\n",
    "    for _, module in unet.named_modules():\n",
    "        if hasattr(module, \"set_processor\"):\n",
    "            module.set_processor(lora_cls(r=r))\n",
    "    return unet\n",
    "\n",
    "def save_unet_lora(unet, save_path):\n",
    "    state = {}\n",
    "    for name, module in unet.named_modules():\n",
    "        proc = getattr(module, \"processor\", None)\n",
    "        if proc is None: continue\n",
    "        for pname, param in module.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                state[f\"{name}.{pname}\"] = param.detach().cpu()\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    save_file(state, save_path)\n",
    "    print(f\"Saved LoRA weights to {save_path}\")\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def encode_long_prompt(prompt, device):\n",
    "    tokens = tokenizer(prompt, truncation=False, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "    # Split into chunks of 77\n",
    "    chunks = tokens[0].split(77)\n",
    "    embeddings = []\n",
    "    for chunk in chunks:\n",
    "        emb = text_encoder(chunk.unsqueeze(0))[0]\n",
    "        embeddings.append(emb)\n",
    "    # Concatenate along sequence dimension\n",
    "    final_emb = torch.cat(embeddings, dim=1)\n",
    "    return final_emb\n",
    "\n",
    "# --- Training loop ---\n",
    "data_dir = \"./data_out\"   # your preprocessed dataset\n",
    "out_dir  = \"./sdxl_lora_out\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "accelerator = Accelerator(mixed_precision=\"fp16\")\n",
    "set_seed(42)\n",
    "\n",
    "pipe = StableDiffusionXLPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "    torch_dtype=torch.float16,\n",
    "    use_safetensors=True,\n",
    ")\n",
    "pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)\n",
    "tokenizer = CLIPTokenizer.from_pretrained(pipe.text_encoder.config._name_or_path)\n",
    "text_encoder = CLIPTextModel.from_pretrained(pipe.text_encoder.config._name_or_path)\n",
    "\n",
    "vae, unet = pipe.vae, pipe.unet\n",
    "vae.requires_grad_(False)\n",
    "inject_lora_unet(unet, r=4)\n",
    "lora_params = [p for p in unet.parameters() if p.requires_grad]\n",
    "\n",
    "dataset = TextImageDataset(data_dir, resolution=512)\n",
    "dl = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "optimizer = torch.optim.AdamW(lora_params, lr=5e-5)\n",
    "unet, optimizer, dl = accelerator.prepare(unet, optimizer, dl)\n",
    "vae.to(accelerator.device)\n",
    "\n",
    "vae_scale_factor = 0.18215\n",
    "global_step, last_save = 0, time.time()\n",
    "\n",
    "for epoch in range(3):   # adjust epochs\n",
    "    for batch in dl:\n",
    "        with accelerator.accumulate(unet):\n",
    "            latents = vae.encode(batch[\"pixel_values\"].to(accelerator.device)).latent_dist.sample()\n",
    "            latents = latents * vae_scale_factor\n",
    "            noise = torch.randn_like(latents)\n",
    "            timesteps = torch.randint(0, pipe.scheduler.config.num_train_timesteps, (latents.shape[0],), device=accelerator.device).long()\n",
    "            noisy_latents = pipe.scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "            prompt_embeds = encode_long_prompt(batch[\"prompt\"], accelerator.device)\n",
    "            model_pred = unet(noisy_latents, timesteps,\n",
    "                                 prompt_embeds=prompt_embeds).sample\n",
    "            loss = F.mse_loss(model_pred, noise)\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        global_step += 1\n",
    "        # Save every 45 minutes\n",
    "        if time.time() - last_save > 45*60:\n",
    "            save_unet_lora(unet, os.path.join(out_dir, f\"lora_step{global_step}.safetensors\"))\n",
    "            last_save = time.time()\n",
    "\n",
    "save_unet_lora(unet, os.path.join(out_dir, \"lora_final.safetensors\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2579914d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-14 08:56:13.462422: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-14 08:56:13.475105: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-12-14 08:56:13.492108: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-12-14 08:56:13.498020: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-12-14 08:56:13.511674: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-12-14 08:56:14.621599: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os, time, math, json\n",
    "import torch, torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from accelerate import Accelerator\n",
    "from accelerate.utils import set_seed\n",
    "from diffusers import StableDiffusionXLPipeline, AutoencoderKL, DDIMScheduler\n",
    "from diffusers.models.attention_processor import LoRAAttnProcessor, LoRAAttnProcessor2_0\n",
    "from safetensors.torch import save_file\n",
    "\n",
    "# --- Dataset ---\n",
    "class TextImageDataset(Dataset):\n",
    "    def __init__(self, data_dir, resolution=512):\n",
    "        self.images_dir = os.path.join(data_dir, \"images\")\n",
    "        self.prompts_path = os.path.join(data_dir, \"prompts.jsonl\")\n",
    "        self.items = [json.loads(line) for line in open(self.prompts_path)]\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((resolution, resolution), interpolation=Image.BICUBIC),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5], [0.5]),\n",
    "        ])\n",
    "    def __len__(self): return len(self.items)\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.items[idx]\n",
    "        img = Image.open(os.path.join(self.images_dir, item[\"image\"])).convert(\"RGB\")\n",
    "        return {\"pixel_values\": self.transform(img), \"prompt\": item[\"prompt\"]}\n",
    "\n",
    "# --- LoRA helpers ---\n",
    "def inject_lora_unet(unet, r=4):\n",
    "    lora_cls = LoRAAttnProcessor2_0 if hasattr(torch, \"compile\") else LoRAAttnProcessor\n",
    "    for _, module in unet.named_modules():\n",
    "        if hasattr(module, \"set_processor\"):\n",
    "            module.set_processor(lora_cls(r=r))\n",
    "    return unet\n",
    "\n",
    "def save_unet_lora(unet, save_path):\n",
    "    state = {}\n",
    "    for name, module in unet.named_modules():\n",
    "        proc = getattr(module, \"processor\", None)\n",
    "        if proc is None: continue\n",
    "        for pname, param in module.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                state[f\"{name}.{pname}\"] = param.detach().cpu()\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    save_file(state, save_path)\n",
    "    print(f\"Saved LoRA weights to {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65da8689",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cf31312c5e248d6bca03fb67b82f251",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "LoRAAttnProcessor2_0.__init__() got an unexpected keyword argument 'r'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m vae, unet \u001b[38;5;241m=\u001b[39m pipe\u001b[38;5;241m.\u001b[39mvae, pipe\u001b[38;5;241m.\u001b[39munet\n\u001b[1;32m     17\u001b[0m vae\u001b[38;5;241m.\u001b[39mrequires_grad_(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 18\u001b[0m \u001b[43minject_lora_unet\u001b[49m\u001b[43m(\u001b[49m\u001b[43munet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m lora_params \u001b[38;5;241m=\u001b[39m [p \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m unet\u001b[38;5;241m.\u001b[39mparameters() \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mrequires_grad]\n\u001b[1;32m     21\u001b[0m dataset \u001b[38;5;241m=\u001b[39m TextImageDataset(data_dir, resolution\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 34\u001b[0m, in \u001b[0;36minject_lora_unet\u001b[0;34m(unet, r)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, module \u001b[38;5;129;01min\u001b[39;00m unet\u001b[38;5;241m.\u001b[39mnamed_modules():\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset_processor\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 34\u001b[0m         module\u001b[38;5;241m.\u001b[39mset_processor(\u001b[43mlora_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m unet\n",
      "\u001b[0;31mTypeError\u001b[0m: LoRAAttnProcessor2_0.__init__() got an unexpected keyword argument 'r'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# --- Training loop ---\n",
    "data_dir = \"./data_out\"   # your preprocessed dataset\n",
    "out_dir  = \"./sdxl_lora_out\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "accelerator = Accelerator(mixed_precision=\"fp16\")\n",
    "set_seed(42)\n",
    "\n",
    "pipe = StableDiffusionXLPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "    torch_dtype=torch.float16,\n",
    "    use_safetensors=True,\n",
    ")\n",
    "pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "vae, unet = pipe.vae, pipe.unet\n",
    "vae.requires_grad_(False)\n",
    "inject_lora_unet(unet, r=4)\n",
    "lora_params = [p for p in unet.parameters() if p.requires_grad]\n",
    "\n",
    "dataset = TextImageDataset(data_dir, resolution=512)\n",
    "dl = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "optimizer = torch.optim.AdamW(lora_params, lr=5e-5)\n",
    "unet, optimizer, dl = accelerator.prepare(unet, optimizer, dl)\n",
    "vae.to(accelerator.device)\n",
    "\n",
    "vae_scale_factor = 0.18215\n",
    "global_step, last_save = 0, time.time()\n",
    "\n",
    "for epoch in range(3):   # adjust epochs\n",
    "    for batch in dl:\n",
    "        with accelerator.accumulate(unet):\n",
    "            latents = vae.encode(batch[\"pixel_values\"].to(accelerator.device)).latent_dist.sample()\n",
    "            latents = latents * vae_scale_factor\n",
    "            noise = torch.randn_like(latents)\n",
    "            timesteps = torch.randint(0, pipe.scheduler.config.num_train_timesteps, (latents.shape[0],), device=accelerator.device).long()\n",
    "            noisy_latents = pipe.scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "            enc = pipe.encode_prompt(batch[\"prompt\"], device=accelerator.device)\n",
    "            model_pred = unet(noisy_latents, timesteps,\n",
    "                              prompt_embeds=enc[\"prompt_embeds\"],\n",
    "                              pooled_prompt_embeds=enc[\"pooled_prompt_embeds\"]).sample\n",
    "            loss = F.mse_loss(model_pred, noise)\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step(); optimizer.zero_grad()\n",
    "\n",
    "        global_step += 1\n",
    "        # Save every 45 minutes\n",
    "        if time.time() - last_save > 45*60:\n",
    "            save_unet_lora(unet, os.path.join(out_dir, f\"lora_step{global_step}.safetensors\"))\n",
    "            last_save = time.time()\n",
    "\n",
    "save_unet_lora(unet, os.path.join(out_dir, \"lora_final.safetensors\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea68bf5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
